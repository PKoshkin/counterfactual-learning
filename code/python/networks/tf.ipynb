{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../general')\n",
    "from pool import Pool\n",
    "from collections import Counter\n",
    "from metric import metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pool = Pool('../data')\n",
    "train_pool, test_pool = pool.train_test_split()\n",
    "train_features = np.concatenate(\n",
    "    (train_pool.features, np.reshape(train_pool.positions, (-1, 1))),\n",
    "    axis=1\n",
    ")\n",
    "train_prediction = np.reshape(train_pool.targets, (-1, 1))\n",
    "test_features = np.concatenate(\n",
    "    (test_pool.features, np.reshape(test_pool.positions, (-1, 1))),\n",
    "    axis=1\n",
    ")\n",
    "test_prediction = np.reshape(test_pool.targets, (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.POSITIONS = list(range(9)) + [100]\n",
    "        num_features = np.shape(train_features)[1]\n",
    "        self.matrix = tf.get_variable(\n",
    "            \"matrix\", shape=(num_features, 1),\n",
    "            initializer=tf.glorot_uniform_initializer()\n",
    "        )\n",
    "        self.bias = tf.get_variable(\n",
    "            \"bias\", shape=(1,),\n",
    "            initializer=tf.glorot_uniform_initializer()\n",
    "        )\n",
    "        self.input_features = tf.placeholder('float32', shape=(None, num_features))\n",
    "        self.input_prediction = tf.placeholder('float32', shape=(None, 1))\n",
    "        self.output_prediction = tf.matmul(self.input_features, self.matrix) + self.bias\n",
    "        self.loss = (\n",
    "            tf.reduce_mean((self.input_prediction - self.output_prediction) ** 2) +\n",
    "            tf.reduce_mean(self.matrix ** 2)\n",
    "        )\n",
    "        self.optimizer = tf.train.AdamOptimizer().minimize(\n",
    "            self.loss, var_list=[self.matrix, self.bias]\n",
    "        )\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.best_marix = self.sess.run(self.matrix)\n",
    "        self.best_bias = self.sess.run(self.bias)\n",
    "\n",
    "    def teach(self, train_features, train_prediction,\n",
    "              test_features, test_prediction, verbose=True, iterations=1000):\n",
    "        best_test_loss = 1e111\n",
    "        for i in range(iterations):\n",
    "            self.sess.run(\n",
    "                self.optimizer, {\n",
    "                    self.input_features: train_features,\n",
    "                    self.input_prediction: train_prediction\n",
    "                }\n",
    "            )\n",
    "            train_loss = self.sess.run(\n",
    "                self.loss, {\n",
    "                    self.input_features: train_features, \n",
    "                    self.input_prediction: train_prediction\n",
    "                }\n",
    "            ),\n",
    "            test_loss = self.sess.run(\n",
    "                self.loss, {\n",
    "                    self.input_features: test_features,\n",
    "                    self.input_prediction: test_prediction\n",
    "                }\n",
    "            ),\n",
    "            test_loss = test_loss[0]\n",
    "            train_loss = train_loss[0]\n",
    "            if test_loss < best_test_loss:\n",
    "                self.best_marix = self.sess.run(self.matrix)\n",
    "                self.best_bias = self.sess.run(self.bias)\n",
    "                best_test_loss = test_loss\n",
    "            if verbose:\n",
    "                print(\n",
    "                    \"train loss: {}, test loss: {}\".format(\n",
    "                        train_loss, test_loss\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    def predict_score(self, features):\n",
    "        return self.sess.run(self.output_prediction, {self.input_features: features})\n",
    "\n",
    "    def predict_positions(self, test_features):\n",
    "        prediction = []\n",
    "        for features in test_features:\n",
    "            max_score = -100\n",
    "            best_position = -100\n",
    "            for position in self.POSITIONS:\n",
    "                new_score = self.predict_score(\n",
    "                    [list(features) + [position]]\n",
    "                )\n",
    "                new_score = new_score[0]\n",
    "                if new_score > max_score:\n",
    "                    max_score = new_score\n",
    "                    best_position = position\n",
    "            prediction.append(best_position)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 186.02474975585938, test loss: 53.172279357910156\n",
      "train loss: 171.31396484375, test loss: 49.420372009277344\n",
      "train loss: 157.787841796875, test loss: 46.3572883605957\n",
      "train loss: 145.28469848632812, test loss: 43.8138313293457\n",
      "train loss: 133.62879943847656, test loss: 41.60847473144531\n",
      "train loss: 122.6788101196289, test loss: 39.59663772583008\n",
      "train loss: 112.34425354003906, test loss: 37.68667984008789\n",
      "train loss: 102.58025360107422, test loss: 35.83430099487305\n",
      "train loss: 93.37439727783203, test loss: 34.02955627441406\n",
      "train loss: 84.73379516601562, test loss: 32.28398132324219\n",
      "train loss: 76.67411804199219, test loss: 30.61935806274414\n",
      "train loss: 69.2108154296875, test loss: 29.05879783630371\n",
      "train loss: 62.3524169921875, test loss: 27.62025260925293\n",
      "train loss: 56.09633255004883, test loss: 26.312158584594727\n",
      "train loss: 50.426822662353516, test loss: 25.131624221801758\n",
      "train loss: 45.316165924072266, test loss: 24.065433502197266\n",
      "train loss: 40.72813415527344, test loss: 23.093708038330078\n",
      "train loss: 36.6231575012207, test loss: 22.195005416870117\n",
      "train loss: 32.96303176879883, test loss: 21.351003646850586\n",
      "train loss: 29.71377182006836, test loss: 20.549348831176758\n",
      "train loss: 26.846391677856445, test loss: 19.78412628173828\n",
      "train loss: 24.335586547851562, test loss: 19.054479598999023\n",
      "train loss: 22.157743453979492, test loss: 18.362245559692383\n",
      "train loss: 20.288625717163086, test loss: 17.709402084350586\n",
      "train loss: 18.701778411865234, test loss: 17.096118927001953\n",
      "train loss: 17.367916107177734, test loss: 16.519805908203125\n",
      "train loss: 16.25546646118164, test loss: 15.975289344787598\n",
      "train loss: 15.332030296325684, test loss: 15.455942153930664\n",
      "train loss: 14.566235542297363, test loss: 14.955216407775879\n",
      "train loss: 13.929476737976074, test loss: 14.468082427978516\n",
      "train loss: 13.396967887878418, test loss: 13.991847038269043\n",
      "train loss: 12.948003768920898, test loss: 13.526182174682617\n",
      "train loss: 12.565424919128418, test loss: 13.0724458694458\n",
      "train loss: 12.234658241271973, test loss: 12.632585525512695\n",
      "train loss: 11.94271469116211, test loss: 12.208056449890137\n",
      "train loss: 11.677488327026367, test loss: 11.799113273620605\n",
      "train loss: 11.42758560180664, test loss: 11.404644012451172\n",
      "train loss: 11.182641983032227, test loss: 11.022551536560059\n",
      "train loss: 10.933974266052246, test loss: 10.6505126953125\n",
      "train loss: 10.675211906433105, test loss: 10.286746978759766\n",
      "train loss: 10.40264892578125, test loss: 9.930519104003906\n",
      "train loss: 10.115180015563965, test loss: 9.582269668579102\n",
      "train loss: 9.813769340515137, test loss: 9.24327564239502\n",
      "train loss: 9.500682830810547, test loss: 8.915082931518555\n",
      "train loss: 9.17868423461914, test loss: 8.598906517028809\n",
      "train loss: 8.85042667388916, test loss: 8.295210838317871\n",
      "train loss: 8.518152236938477, test loss: 8.00361442565918\n",
      "train loss: 8.183758735656738, test loss: 7.7231059074401855\n",
      "train loss: 7.849045753479004, test loss: 7.45245361328125\n",
      "train loss: 7.516014575958252, test loss: 7.190637588500977\n",
      "train loss: 7.187038898468018, test loss: 6.937132835388184\n",
      "train loss: 6.864820957183838, test loss: 6.6919474601745605\n",
      "train loss: 6.552154541015625, test loss: 6.45545768737793\n",
      "train loss: 6.251576900482178, test loss: 6.228115558624268\n",
      "train loss: 5.965067386627197, test loss: 6.010164260864258\n",
      "train loss: 5.6938605308532715, test loss: 5.801485538482666\n",
      "train loss: 5.438466548919678, test loss: 5.601604461669922\n",
      "train loss: 5.198824405670166, test loss: 5.409846305847168\n",
      "train loss: 4.9745283126831055, test loss: 5.22552490234375\n",
      "train loss: 4.765037536621094, test loss: 5.048135757446289\n",
      "train loss: 4.569791793823242, test loss: 4.877414703369141\n",
      "train loss: 4.388216018676758, test loss: 4.713305950164795\n",
      "train loss: 4.219661712646484, test loss: 4.555849552154541\n",
      "train loss: 4.063329696655273, test loss: 4.405050754547119\n",
      "train loss: 3.9182331562042236, test loss: 4.260791778564453\n",
      "train loss: 3.7832329273223877, test loss: 4.12282657623291\n",
      "train loss: 3.6571247577667236, test loss: 3.99082350730896\n",
      "train loss: 3.5387496948242188, test loss: 3.864445686340332\n",
      "train loss: 3.427083969116211, test loss: 3.743415355682373\n",
      "train loss: 3.321274757385254, test loss: 3.627535581588745\n",
      "train loss: 3.2206292152404785, test loss: 3.5166666507720947\n",
      "train loss: 3.124565601348877, test loss: 3.4106791019439697\n",
      "train loss: 3.0325684547424316, test loss: 3.3094093799591064\n",
      "train loss: 2.9441609382629395, test loss: 3.2126381397247314\n",
      "train loss: 2.858912467956543, test loss: 3.120112657546997\n",
      "train loss: 2.7764649391174316, test loss: 3.031576633453369\n",
      "train loss: 2.6965596675872803, test loss: 2.946812629699707\n",
      "train loss: 2.619046688079834, test loss: 2.865661144256592\n",
      "train loss: 2.5438709259033203, test loss: 2.7880160808563232\n",
      "train loss: 2.471036911010742, test loss: 2.7137951850891113\n",
      "train loss: 2.4005696773529053, test loss: 2.642911672592163\n",
      "train loss: 2.3324854373931885, test loss: 2.575249195098877\n",
      "train loss: 2.26678466796875, test loss: 2.510662794113159\n",
      "train loss: 2.203458070755005, test loss: 2.4489877223968506\n",
      "train loss: 2.142500877380371, test loss: 2.3900630474090576\n",
      "train loss: 2.0839223861694336, test loss: 2.3337433338165283\n",
      "train loss: 2.027742862701416, test loss: 2.2799041271209717\n",
      "train loss: 1.9739817380905151, test loss: 2.2284326553344727\n",
      "train loss: 1.922637939453125, test loss: 2.1792147159576416\n",
      "train loss: 1.8736791610717773, test loss: 2.132127046585083\n",
      "train loss: 1.82703697681427, test loss: 2.0870351791381836\n",
      "train loss: 1.7826169729232788, test loss: 2.04380464553833\n",
      "train loss: 1.7403104305267334, test loss: 2.0023131370544434\n",
      "train loss: 1.7000079154968262, test loss: 1.9624611139297485\n",
      "train loss: 1.6616042852401733, test loss: 1.9241760969161987\n",
      "train loss: 1.6249996423721313, test loss: 1.8874045610427856\n",
      "train loss: 1.5900930166244507, test loss: 1.8521051406860352\n",
      "train loss: 1.556778907775879, test loss: 1.8182344436645508\n",
      "train loss: 1.524946928024292, test loss: 1.7857460975646973\n",
      "train loss: 1.4944857358932495, test loss: 1.75458562374115\n",
      "train loss: 1.4652888774871826, test loss: 1.7246966361999512\n",
      "train loss: 1.4372614622116089, test loss: 1.6960220336914062\n",
      "train loss: 1.4103213548660278, test loss: 1.668506383895874\n",
      "train loss: 1.3843985795974731, test loss: 1.642095923423767\n",
      "train loss: 1.3594330549240112, test loss: 1.6167360544204712\n",
      "train loss: 1.335369348526001, test loss: 1.5923703908920288\n",
      "train loss: 1.312156319618225, test loss: 1.5689419507980347\n",
      "train loss: 1.289747714996338, test loss: 1.5463956594467163\n",
      "train loss: 1.2681024074554443, test loss: 1.5246827602386475\n",
      "train loss: 1.247186303138733, test loss: 1.5037610530853271\n",
      "train loss: 1.2269710302352905, test loss: 1.4835963249206543\n",
      "train loss: 1.207431674003601, test loss: 1.4641612768173218\n",
      "train loss: 1.1885461807250977, test loss: 1.4454314708709717\n",
      "train loss: 1.1702914237976074, test loss: 1.4273844957351685\n",
      "train loss: 1.152644157409668, test loss: 1.4099959135055542\n",
      "train loss: 1.1355818510055542, test loss: 1.3932430744171143\n",
      "train loss: 1.1190818548202515, test loss: 1.3771015405654907\n",
      "train loss: 1.1031230688095093, test loss: 1.3615483045578003\n",
      "train loss: 1.0876851081848145, test loss: 1.3465583324432373\n",
      "train loss: 1.0727474689483643, test loss: 1.3321073055267334\n",
      "train loss: 1.0582890510559082, test loss: 1.318169355392456\n",
      "train loss: 1.0442878007888794, test loss: 1.304718255996704\n",
      "train loss: 1.0307214260101318, test loss: 1.2917283773422241\n",
      "train loss: 1.017567753791809, test loss: 1.2791752815246582\n",
      "train loss: 1.0048058032989502, test loss: 1.2670373916625977\n",
      "train loss: 0.9924161434173584, test loss: 1.2552952766418457\n",
      "train loss: 0.980380117893219, test loss: 1.2439314126968384\n",
      "train loss: 0.9686801433563232, test loss: 1.2329299449920654\n",
      "train loss: 0.9572991728782654, test loss: 1.2222743034362793\n",
      "train loss: 0.9462205767631531, test loss: 1.211949110031128\n",
      "train loss: 0.9354292750358582, test loss: 1.2019388675689697\n",
      "train loss: 0.9249107241630554, test loss: 1.1922283172607422\n",
      "train loss: 0.9146525859832764, test loss: 1.1828020811080933\n",
      "train loss: 0.9046434164047241, test loss: 1.1736458539962769\n",
      "train loss: 0.8948727250099182, test loss: 1.1647462844848633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.8853310346603394, test loss: 1.1560897827148438\n",
      "train loss: 0.8760091662406921, test loss: 1.1476653814315796\n",
      "train loss: 0.8668987154960632, test loss: 1.1394611597061157\n",
      "train loss: 0.8579916954040527, test loss: 1.1314677000045776\n",
      "train loss: 0.8492807149887085, test loss: 1.1236766576766968\n",
      "train loss: 0.8407590389251709, test loss: 1.1160808801651\n",
      "train loss: 0.832420289516449, test loss: 1.108674168586731\n",
      "train loss: 0.8242583274841309, test loss: 1.1014492511749268\n",
      "train loss: 0.8162670135498047, test loss: 1.0944002866744995\n",
      "train loss: 0.8084407448768616, test loss: 1.087520718574524\n",
      "train loss: 0.800773561000824, test loss: 1.0808030366897583\n",
      "train loss: 0.7932602763175964, test loss: 1.0742403268814087\n",
      "train loss: 0.7858956456184387, test loss: 1.067825198173523\n",
      "train loss: 0.7786746025085449, test loss: 1.0615497827529907\n",
      "train loss: 0.7715923190116882, test loss: 1.0554070472717285\n",
      "train loss: 0.7646443843841553, test loss: 1.0493910312652588\n",
      "train loss: 0.7578259706497192, test loss: 1.0434951782226562\n",
      "train loss: 0.7511328458786011, test loss: 1.0377146005630493\n",
      "train loss: 0.7445610761642456, test loss: 1.0320444107055664\n",
      "train loss: 0.7381067872047424, test loss: 1.0264812707901\n",
      "train loss: 0.7317662239074707, test loss: 1.0210216045379639\n",
      "train loss: 0.7255359888076782, test loss: 1.0156625509262085\n",
      "train loss: 0.7194128632545471, test loss: 1.0104013681411743\n",
      "train loss: 0.7133938074111938, test loss: 1.0052345991134644\n",
      "train loss: 0.7074756026268005, test loss: 1.0001591444015503\n",
      "train loss: 0.7016556859016418, test loss: 0.9951715469360352\n",
      "train loss: 0.6959313154220581, test loss: 0.990267813205719\n",
      "train loss: 0.6903001070022583, test loss: 0.9854446649551392\n",
      "train loss: 0.6847596168518066, test loss: 0.9806985855102539\n",
      "train loss: 0.6793076395988464, test loss: 0.9760267734527588\n",
      "train loss: 0.6739420294761658, test loss: 0.9714251756668091\n",
      "train loss: 0.6686605215072632, test loss: 0.9668917059898376\n",
      "train loss: 0.6634612679481506, test loss: 0.9624244570732117\n",
      "train loss: 0.6583422422409058, test loss: 0.9580214023590088\n",
      "train loss: 0.6533017158508301, test loss: 0.9536812901496887\n",
      "train loss: 0.6483376622200012, test loss: 0.9494027495384216\n",
      "train loss: 0.6434486508369446, test loss: 0.9451847672462463\n",
      "train loss: 0.6386327743530273, test loss: 0.9410258531570435\n",
      "train loss: 0.6338883638381958, test loss: 0.9369246363639832\n",
      "train loss: 0.6292139291763306, test loss: 0.9328798651695251\n",
      "train loss: 0.6246079802513123, test loss: 0.928889274597168\n",
      "train loss: 0.6200689673423767, test loss: 0.9249516725540161\n",
      "train loss: 0.6155953407287598, test loss: 0.9210646748542786\n",
      "train loss: 0.6111859679222107, test loss: 0.9172271490097046\n",
      "train loss: 0.6068393588066101, test loss: 0.9134365320205688\n",
      "train loss: 0.6025540828704834, test loss: 0.9096918106079102\n",
      "train loss: 0.5983291864395142, test loss: 0.9059910178184509\n",
      "train loss: 0.5941632390022278, test loss: 0.9023332595825195\n",
      "train loss: 0.5900551676750183, test loss: 0.898716926574707\n",
      "train loss: 0.5860037207603455, test loss: 0.8951408267021179\n",
      "train loss: 0.5820078253746033, test loss: 0.8916040062904358\n",
      "train loss: 0.5780665874481201, test loss: 0.8881053328514099\n",
      "train loss: 0.5741787552833557, test loss: 0.8846438527107239\n",
      "train loss: 0.5703434944152832, test loss: 0.881218671798706\n",
      "train loss: 0.5665596127510071, test loss: 0.8778277039527893\n",
      "train loss: 0.5628262758255005, test loss: 0.8744707703590393\n",
      "train loss: 0.5591426491737366, test loss: 0.8711466789245605\n",
      "train loss: 0.5555077195167542, test loss: 0.867854118347168\n",
      "train loss: 0.5519205331802368, test loss: 0.8645926713943481\n",
      "train loss: 0.5483803153038025, test loss: 0.8613609075546265\n",
      "train loss: 0.5448862910270691, test loss: 0.8581582307815552\n",
      "train loss: 0.5414376258850098, test loss: 0.8549841046333313\n",
      "train loss: 0.5380333065986633, test loss: 0.8518381118774414\n",
      "train loss: 0.5346728563308716, test loss: 0.8487187623977661\n",
      "train loss: 0.5313553214073181, test loss: 0.8456259965896606\n",
      "train loss: 0.5280799269676208, test loss: 0.8425590991973877\n",
      "train loss: 0.5248461365699768, test loss: 0.8395172953605652\n",
      "train loss: 0.5216531157493591, test loss: 0.8364997506141663\n",
      "train loss: 0.5185000896453857, test loss: 0.8335065245628357\n",
      "train loss: 0.5153864622116089, test loss: 0.8305363655090332\n",
      "train loss: 0.5123115181922913, test loss: 0.8275889754295349\n",
      "train loss: 0.5092747807502747, test loss: 0.8246635794639587\n",
      "train loss: 0.506275475025177, test loss: 0.8217606544494629\n",
      "train loss: 0.5033130049705505, test loss: 0.8188785314559937\n",
      "train loss: 0.500386655330658, test loss: 0.8160175681114197\n",
      "train loss: 0.4974960684776306, test loss: 0.8131770491600037\n",
      "train loss: 0.49464040994644165, test loss: 0.8103564381599426\n",
      "train loss: 0.4918192923069, test loss: 0.8075557351112366\n",
      "train loss: 0.4890320897102356, test loss: 0.8047741055488586\n",
      "train loss: 0.4862782061100006, test loss: 0.8020113706588745\n",
      "train loss: 0.4835572838783264, test loss: 0.7992665767669678\n",
      "train loss: 0.4808685779571533, test loss: 0.796540379524231\n",
      "train loss: 0.4782117009162903, test loss: 0.7938312888145447\n",
      "train loss: 0.4755861163139343, test loss: 0.7911394238471985\n",
      "train loss: 0.47299134731292725, test loss: 0.7884644269943237\n",
      "train loss: 0.47042688727378845, test loss: 0.7858059406280518\n",
      "train loss: 0.4678923189640045, test loss: 0.7831637263298035\n",
      "train loss: 0.4653870761394501, test loss: 0.7805376052856445\n",
      "train loss: 0.4629107713699341, test loss: 0.7779269814491272\n",
      "train loss: 0.4604630172252655, test loss: 0.775331974029541\n",
      "train loss: 0.45804324746131897, test loss: 0.7727521657943726\n",
      "train loss: 0.4556511342525482, test loss: 0.7701870799064636\n",
      "train loss: 0.45328617095947266, test loss: 0.7676368951797485\n",
      "train loss: 0.450948029756546, test loss: 0.7651013135910034\n",
      "train loss: 0.44863632321357727, test loss: 0.7625796794891357\n",
      "train loss: 0.44635045528411865, test loss: 0.7600720524787903\n",
      "train loss: 0.444090336561203, test loss: 0.7575779557228088\n",
      "train loss: 0.4418553411960602, test loss: 0.7550973892211914\n",
      "train loss: 0.4396452307701111, test loss: 0.7526298761367798\n",
      "train loss: 0.4374595582485199, test loss: 0.7501754760742188\n",
      "train loss: 0.43529796600341797, test loss: 0.7477337718009949\n",
      "train loss: 0.4331602156162262, test loss: 0.7453048825263977\n",
      "train loss: 0.4310457408428192, test loss: 0.7428886294364929\n",
      "train loss: 0.42895442247390747, test loss: 0.7404845952987671\n",
      "train loss: 0.42688581347465515, test loss: 0.7380926012992859\n",
      "train loss: 0.424839586019516, test loss: 0.7357123494148254\n",
      "train loss: 0.4228154420852661, test loss: 0.7333440780639648\n",
      "train loss: 0.42081305384635925, test loss: 0.730987548828125\n",
      "train loss: 0.41883212327957153, test loss: 0.728642463684082\n",
      "train loss: 0.4168722927570343, test loss: 0.726308286190033\n",
      "train loss: 0.41493335366249084, test loss: 0.7239855527877808\n",
      "train loss: 0.4130149185657501, test loss: 0.7216735482215881\n",
      "train loss: 0.41111674904823303, test loss: 0.7193726897239685\n",
      "train loss: 0.4092385172843933, test loss: 0.717082142829895\n",
      "train loss: 0.40737998485565186, test loss: 0.7148028016090393\n",
      "train loss: 0.4055408239364624, test loss: 0.712533712387085\n",
      "train loss: 0.4037208557128906, test loss: 0.710274875164032\n",
      "train loss: 0.40191975235939026, test loss: 0.7080264091491699\n",
      "train loss: 0.4001372754573822, test loss: 0.7057881951332092\n",
      "train loss: 0.3983730673789978, test loss: 0.7035601735115051\n",
      "train loss: 0.3966270089149475, test loss: 0.7013421654701233\n",
      "train loss: 0.39489880204200745, test loss: 0.699134111404419\n",
      "train loss: 0.3931881785392761, test loss: 0.6969354748725891\n",
      "train loss: 0.3914949297904968, test loss: 0.6947469711303711\n",
      "train loss: 0.38981887698173523, test loss: 0.6925679445266724\n",
      "train loss: 0.3881596624851227, test loss: 0.6903987526893616\n",
      "train loss: 0.38651710748672485, test loss: 0.6882385015487671\n",
      "train loss: 0.38489100337028503, test loss: 0.6860879063606262\n",
      "train loss: 0.3832811117172241, test loss: 0.6839467287063599\n",
      "train loss: 0.3816872537136078, test loss: 0.6818146705627441\n",
      "train loss: 0.3801092207431793, test loss: 0.6796916127204895\n",
      "train loss: 0.37854674458503723, test loss: 0.677577793598175\n",
      "train loss: 0.3769995868206024, test loss: 0.6754729747772217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.3754676580429077, test loss: 0.6733771562576294\n",
      "train loss: 0.37395069003105164, test loss: 0.67128986120224\n",
      "train loss: 0.37244853377342224, test loss: 0.669211745262146\n",
      "train loss: 0.37096095085144043, test loss: 0.6671422123908997\n",
      "train loss: 0.3694877326488495, test loss: 0.6650813817977905\n",
      "train loss: 0.3680287003517151, test loss: 0.6630290150642395\n",
      "train loss: 0.36658376455307007, test loss: 0.6609850525856018\n",
      "train loss: 0.36515265703201294, test loss: 0.6589497923851013\n",
      "train loss: 0.363735169172287, test loss: 0.6569229364395142\n",
      "train loss: 0.36233118176460266, test loss: 0.6549041867256165\n",
      "train loss: 0.360940545797348, test loss: 0.652894139289856\n",
      "train loss: 0.3595629930496216, test loss: 0.6508920192718506\n",
      "train loss: 0.35819846391677856, test loss: 0.648898184299469\n",
      "train loss: 0.35684677958488464, test loss: 0.6469125747680664\n",
      "train loss: 0.35550767183303833, test loss: 0.6449349522590637\n",
      "train loss: 0.35418105125427246, test loss: 0.6429653167724609\n",
      "train loss: 0.35286685824394226, test loss: 0.6410037875175476\n",
      "train loss: 0.35156479477882385, test loss: 0.6390502452850342\n",
      "train loss: 0.3502747714519501, test loss: 0.6371046900749207\n",
      "train loss: 0.3489965796470642, test loss: 0.6351666450500488\n",
      "train loss: 0.3477300703525543, test loss: 0.6332369446754456\n",
      "train loss: 0.3464752435684204, test loss: 0.6313146352767944\n",
      "train loss: 0.345231831073761, test loss: 0.6294000148773193\n",
      "train loss: 0.3439997434616089, test loss: 0.6274933218955994\n",
      "train loss: 0.3427787721157074, test loss: 0.6255943179130554\n",
      "train loss: 0.34156882762908936, test loss: 0.6237028241157532\n",
      "train loss: 0.34036985039711, test loss: 0.6218187808990479\n",
      "train loss: 0.3391815721988678, test loss: 0.6199424266815186\n",
      "train loss: 0.33800384402275085, test loss: 0.6180734038352966\n",
      "train loss: 0.33683672547340393, test loss: 0.616212010383606\n",
      "train loss: 0.3356799781322479, test loss: 0.6143577694892883\n",
      "train loss: 0.3345334231853485, test loss: 0.612511157989502\n",
      "train loss: 0.3333970010280609, test loss: 0.6106719374656677\n",
      "train loss: 0.3322705924510956, test loss: 0.6088398098945618\n",
      "train loss: 0.3311540484428406, test loss: 0.6070151329040527\n",
      "train loss: 0.3300473093986511, test loss: 0.6051979064941406\n",
      "train loss: 0.3289501965045929, test loss: 0.6033875942230225\n",
      "train loss: 0.32786262035369873, test loss: 0.6015845537185669\n",
      "train loss: 0.3267844617366791, test loss: 0.5997887253761292\n",
      "train loss: 0.3257156312465668, test loss: 0.5979997515678406\n",
      "train loss: 0.3246559798717499, test loss: 0.5962185859680176\n",
      "train loss: 0.323605477809906, test loss: 0.594443678855896\n",
      "train loss: 0.32256391644477844, test loss: 0.5926761031150818\n",
      "train loss: 0.3215312659740448, test loss: 0.5909154415130615\n",
      "train loss: 0.32050731778144836, test loss: 0.5891619324684143\n",
      "train loss: 0.3194921016693115, test loss: 0.5874152183532715\n",
      "train loss: 0.3184855282306671, test loss: 0.5856753587722778\n",
      "train loss: 0.31748732924461365, test loss: 0.5839423537254333\n",
      "train loss: 0.3164975941181183, test loss: 0.5822164416313171\n",
      "train loss: 0.31551608443260193, test loss: 0.5804969668388367\n",
      "train loss: 0.3145428001880646, test loss: 0.5787844657897949\n",
      "train loss: 0.3135775923728943, test loss: 0.577078640460968\n",
      "train loss: 0.3126204311847687, test loss: 0.5753797292709351\n",
      "train loss: 0.3116711378097534, test loss: 0.5736872553825378\n",
      "train loss: 0.3107297122478485, test loss: 0.572001576423645\n",
      "train loss: 0.309796005487442, test loss: 0.570322573184967\n",
      "train loss: 0.30886998772621155, test loss: 0.5686504244804382\n",
      "train loss: 0.3079514801502228, test loss: 0.5669845938682556\n",
      "train loss: 0.30704042315483093, test loss: 0.5653254389762878\n",
      "train loss: 0.3061368763446808, test loss: 0.563672661781311\n",
      "train loss: 0.30524057149887085, test loss: 0.5620263814926147\n",
      "train loss: 0.3043515384197235, test loss: 0.5603867173194885\n",
      "train loss: 0.3034696578979492, test loss: 0.5587535500526428\n",
      "train loss: 0.3025948107242584, test loss: 0.5571266412734985\n",
      "train loss: 0.30172696709632874, test loss: 0.5555062294006348\n",
      "train loss: 0.3008660674095154, test loss: 0.5538920760154724\n",
      "train loss: 0.3000120520591736, test loss: 0.5522843599319458\n",
      "train loss: 0.299164742231369, test loss: 0.5506829619407654\n",
      "train loss: 0.2983241677284241, test loss: 0.5490878820419312\n",
      "train loss: 0.2974902093410492, test loss: 0.5474990606307983\n",
      "train loss: 0.2966627776622772, test loss: 0.5459164977073669\n",
      "train loss: 0.29584187269210815, test loss: 0.5443400144577026\n",
      "train loss: 0.29502734541893005, test loss: 0.5427699089050293\n",
      "train loss: 0.2942192256450653, test loss: 0.5412060022354126\n",
      "train loss: 0.2934172749519348, test loss: 0.539648175239563\n",
      "train loss: 0.29262158274650574, test loss: 0.5380964875221252\n",
      "train loss: 0.2918320596218109, test loss: 0.536550760269165\n",
      "train loss: 0.2910485863685608, test loss: 0.5350110530853271\n",
      "train loss: 0.290271133184433, test loss: 0.5334774255752563\n",
      "train loss: 0.28949958086013794, test loss: 0.531950056552887\n",
      "train loss: 0.2887340188026428, test loss: 0.5304283499717712\n",
      "train loss: 0.28797417879104614, test loss: 0.5289129614830017\n",
      "train loss: 0.28722015023231506, test loss: 0.5274031162261963\n",
      "train loss: 0.2864718437194824, test loss: 0.5258994698524475\n",
      "train loss: 0.28572914004325867, test loss: 0.5244014263153076\n",
      "train loss: 0.2849920392036438, test loss: 0.5229095816612244\n",
      "train loss: 0.28426045179367065, test loss: 0.5214235186576843\n",
      "train loss: 0.28353434801101685, test loss: 0.519943118095398\n",
      "train loss: 0.2828136682510376, test loss: 0.5184686183929443\n",
      "train loss: 0.28209832310676575, test loss: 0.5169997215270996\n",
      "train loss: 0.2813882529735565, test loss: 0.515536904335022\n",
      "train loss: 0.2806834578514099, test loss: 0.514079213142395\n",
      "train loss: 0.27998387813568115, test loss: 0.5126274824142456\n",
      "train loss: 0.2792893946170807, test loss: 0.5111815333366394\n",
      "train loss: 0.2786000072956085, test loss: 0.5097412467002869\n",
      "train loss: 0.27791568636894226, test loss: 0.5083064436912537\n",
      "train loss: 0.27723631262779236, test loss: 0.5068773627281189\n",
      "train loss: 0.2765619158744812, test loss: 0.505453884601593\n",
      "train loss: 0.27589234709739685, test loss: 0.5040356516838074\n",
      "train loss: 0.2752276659011841, test loss: 0.5026230812072754\n",
      "train loss: 0.27456775307655334, test loss: 0.5012161135673523\n",
      "train loss: 0.27391257882118225, test loss: 0.49981454014778137\n",
      "train loss: 0.2732621133327484, test loss: 0.4984184205532074\n",
      "train loss: 0.2726162374019623, test loss: 0.49702751636505127\n",
      "train loss: 0.27197501063346863, test loss: 0.4956422746181488\n",
      "train loss: 0.2713383436203003, test loss: 0.49426233768463135\n",
      "train loss: 0.2707061767578125, test loss: 0.49288761615753174\n",
      "train loss: 0.2700784504413605, test loss: 0.49151840806007385\n",
      "train loss: 0.2694551348686218, test loss: 0.4901542067527771\n",
      "train loss: 0.26883620023727417, test loss: 0.4887954592704773\n",
      "train loss: 0.2682216465473175, test loss: 0.4874420166015625\n",
      "train loss: 0.2676113247871399, test loss: 0.48609355092048645\n",
      "train loss: 0.26700523495674133, test loss: 0.48475056886672974\n",
      "train loss: 0.2664034068584442, test loss: 0.48341265320777893\n",
      "train loss: 0.26580575108528137, test loss: 0.4820798933506012\n",
      "train loss: 0.26521217823028564, test loss: 0.48075222969055176\n",
      "train loss: 0.2646227180957794, test loss: 0.4794297218322754\n",
      "train loss: 0.2640373110771179, test loss: 0.4781121611595154\n",
      "train loss: 0.2634558379650116, test loss: 0.4767999053001404\n",
      "train loss: 0.2628784477710724, test loss: 0.47549229860305786\n",
      "train loss: 0.2623049020767212, test loss: 0.4741901755332947\n",
      "train loss: 0.2617352604866028, test loss: 0.4728926420211792\n",
      "train loss: 0.26116952300071716, test loss: 0.4716002345085144\n",
      "train loss: 0.2606075406074524, test loss: 0.4703127145767212\n",
      "train loss: 0.26004937291145325, test loss: 0.4690301716327667\n",
      "train loss: 0.25949496030807495, test loss: 0.4677524268627167\n",
      "train loss: 0.25894424319267273, test loss: 0.46647974848747253\n",
      "train loss: 0.2583971917629242, test loss: 0.4652116000652313\n",
      "train loss: 0.25785380601882935, test loss: 0.4639486074447632\n",
      "train loss: 0.2573140263557434, test loss: 0.462690144777298\n",
      "train loss: 0.256777822971344, test loss: 0.46143653988838196\n",
      "train loss: 0.2562451660633087, test loss: 0.4601878821849823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.255715936422348, test loss: 0.458943635225296\n",
      "train loss: 0.25519028306007385, test loss: 0.4577043056488037\n",
      "train loss: 0.2546680271625519, test loss: 0.45646968483924866\n",
      "train loss: 0.2541491985321045, test loss: 0.4552396237850189\n",
      "train loss: 0.2536337673664093, test loss: 0.4540143311023712\n",
      "train loss: 0.25312161445617676, test loss: 0.4527934193611145\n",
      "train loss: 0.252612829208374, test loss: 0.4515772759914398\n",
      "train loss: 0.25210732221603394, test loss: 0.45036572217941284\n",
      "train loss: 0.2516050636768341, test loss: 0.4491584300994873\n",
      "train loss: 0.25110602378845215, test loss: 0.4479561746120453\n",
      "train loss: 0.25061023235321045, test loss: 0.44675812125205994\n",
      "train loss: 0.25011754035949707, test loss: 0.4455646276473999\n",
      "train loss: 0.24962802231311798, test loss: 0.44437548518180847\n",
      "train loss: 0.2491416186094284, test loss: 0.4431908428668976\n",
      "train loss: 0.24865829944610596, test loss: 0.4420106112957001\n",
      "train loss: 0.24817800521850586, test loss: 0.4408348798751831\n",
      "train loss: 0.2477007359266281, test loss: 0.43966349959373474\n",
      "train loss: 0.24722647666931152, test loss: 0.438496470451355\n",
      "train loss: 0.2467552125453949, test loss: 0.4373336732387543\n",
      "train loss: 0.24628688395023346, test loss: 0.4361753761768341\n",
      "train loss: 0.24582146108150482, test loss: 0.43502113223075867\n",
      "train loss: 0.24535895884037018, test loss: 0.43387144804000854\n",
      "train loss: 0.24489928781986237, test loss: 0.43272578716278076\n",
      "train loss: 0.24444249272346497, test loss: 0.4315846264362335\n",
      "train loss: 0.2439885288476944, test loss: 0.4304473400115967\n",
      "train loss: 0.2435373216867447, test loss: 0.4293145537376404\n",
      "train loss: 0.24308888614177704, test loss: 0.4281856417655945\n",
      "train loss: 0.24264320731163025, test loss: 0.4270610213279724\n",
      "train loss: 0.24220025539398193, test loss: 0.42594069242477417\n",
      "train loss: 0.2417600005865097, test loss: 0.4248242676258087\n",
      "train loss: 0.24132241308689117, test loss: 0.4237119257450104\n",
      "train loss: 0.24088747799396515, test loss: 0.4226038455963135\n",
      "train loss: 0.24045516550540924, test loss: 0.42149975895881653\n",
      "train loss: 0.24002544581890106, test loss: 0.4203997552394867\n",
      "train loss: 0.2395983636379242, test loss: 0.4193035960197449\n",
      "train loss: 0.2391737699508667, test loss: 0.4182116687297821\n",
      "train loss: 0.23875172436237335, test loss: 0.41712358593940735\n",
      "train loss: 0.23833222687244415, test loss: 0.4160393178462982\n",
      "train loss: 0.23791520297527313, test loss: 0.4149591326713562\n",
      "train loss: 0.23750066757202148, test loss: 0.4138829708099365\n",
      "train loss: 0.23708856105804443, test loss: 0.4128105938434601\n",
      "train loss: 0.23667888343334198, test loss: 0.4117422103881836\n",
      "train loss: 0.23627164959907532, test loss: 0.41067758202552795\n",
      "train loss: 0.23586678504943848, test loss: 0.40961676836013794\n",
      "train loss: 0.23546428978443146, test loss: 0.40855976939201355\n",
      "train loss: 0.23506411910057068, test loss: 0.4075067341327667\n",
      "train loss: 0.23466633260250092, test loss: 0.4064573645591736\n",
      "train loss: 0.23427079617977142, test loss: 0.40541189908981323\n",
      "train loss: 0.23387758433818817, test loss: 0.4043700397014618\n",
      "train loss: 0.2334866225719452, test loss: 0.403331995010376\n",
      "train loss: 0.23309794068336487, test loss: 0.4022977948188782\n",
      "train loss: 0.23271147906780243, test loss: 0.4012671411037445\n",
      "train loss: 0.23232723772525787, test loss: 0.4002402126789093\n",
      "train loss: 0.23194520175457, test loss: 0.3992168605327606\n",
      "train loss: 0.23156532645225525, test loss: 0.3981971740722656\n",
      "train loss: 0.2311876267194748, test loss: 0.3971813917160034\n",
      "train loss: 0.23081207275390625, test loss: 0.3961690068244934\n",
      "train loss: 0.23043863475322723, test loss: 0.3951602280139923\n",
      "train loss: 0.23006731271743774, test loss: 0.3941550850868225\n",
      "train loss: 0.229698047041893, test loss: 0.393153578042984\n",
      "train loss: 0.229330912232399, test loss: 0.3921554982662201\n",
      "train loss: 0.22896578907966614, test loss: 0.3911610245704651\n",
      "train loss: 0.22860272228717804, test loss: 0.390170156955719\n",
      "train loss: 0.2282417118549347, test loss: 0.38918283581733704\n",
      "train loss: 0.22788265347480774, test loss: 0.3881988227367401\n",
      "train loss: 0.22752560675144196, test loss: 0.3872183561325073\n",
      "train loss: 0.22717054188251495, test loss: 0.3862413465976715\n",
      "train loss: 0.22681744396686554, test loss: 0.3852679431438446\n",
      "train loss: 0.22646626830101013, test loss: 0.38429784774780273\n",
      "train loss: 0.2261170595884323, test loss: 0.38333097100257874\n",
      "train loss: 0.22576969861984253, test loss: 0.38236767053604126\n",
      "train loss: 0.22542427480220795, test loss: 0.3814077079296112\n",
      "train loss: 0.22508075833320618, test loss: 0.3804512023925781\n",
      "train loss: 0.22473908960819244, test loss: 0.37949803471565247\n",
      "train loss: 0.22439926862716675, test loss: 0.37854820489883423\n",
      "train loss: 0.2240612804889679, test loss: 0.37760159373283386\n",
      "train loss: 0.2237251251935959, test loss: 0.3766583502292633\n",
      "train loss: 0.22339077293872833, test loss: 0.3757185637950897\n",
      "train loss: 0.22305822372436523, test loss: 0.37478190660476685\n",
      "train loss: 0.2227274477481842, test loss: 0.3738485872745514\n",
      "train loss: 0.22239847481250763, test loss: 0.3729184865951538\n",
      "train loss: 0.22207121551036835, test loss: 0.37199145555496216\n",
      "train loss: 0.22174571454524994, test loss: 0.3710677921772003\n",
      "train loss: 0.22142194211483002, test loss: 0.37014737725257874\n",
      "train loss: 0.2210998833179474, test loss: 0.36923009157180786\n",
      "train loss: 0.22077953815460205, test loss: 0.3683162331581116\n",
      "train loss: 0.22046086192131042, test loss: 0.36740532517433167\n",
      "train loss: 0.2201438844203949, test loss: 0.36649754643440247\n",
      "train loss: 0.2198285460472107, test loss: 0.3655930757522583\n",
      "train loss: 0.219514861702919, test loss: 0.3646916151046753\n",
      "train loss: 0.21920281648635864, test loss: 0.363793283700943\n",
      "train loss: 0.21889238059520721, test loss: 0.36289799213409424\n",
      "train loss: 0.2185835987329483, test loss: 0.36200597882270813\n",
      "train loss: 0.21827638149261475, test loss: 0.36111682653427124\n",
      "train loss: 0.21797078847885132, test loss: 0.360230952501297\n",
      "train loss: 0.21766673028469086, test loss: 0.359347939491272\n",
      "train loss: 0.21736426651477814, test loss: 0.3584682047367096\n",
      "train loss: 0.21706333756446838, test loss: 0.35759130120277405\n",
      "train loss: 0.2167639583349228, test loss: 0.35671764612197876\n",
      "train loss: 0.21646611392498016, test loss: 0.3558465838432312\n",
      "train loss: 0.2161698043346405, test loss: 0.35497885942459106\n",
      "train loss: 0.21587499976158142, test loss: 0.3541141152381897\n",
      "train loss: 0.21558165550231934, test loss: 0.3532521426677704\n",
      "train loss: 0.21528981626033783, test loss: 0.35239318013191223\n",
      "train loss: 0.2149994820356369, test loss: 0.3515373170375824\n",
      "train loss: 0.21471059322357178, test loss: 0.35068437457084656\n",
      "train loss: 0.21442313492298126, test loss: 0.3498342037200928\n",
      "train loss: 0.21413712203502655, test loss: 0.34898707270622253\n",
      "train loss: 0.21385255455970764, test loss: 0.3481428027153015\n",
      "train loss: 0.21356941759586334, test loss: 0.3473013937473297\n",
      "train loss: 0.21328769624233246, test loss: 0.3464628756046295\n",
      "train loss: 0.2130073606967926, test loss: 0.3456272482872009\n",
      "train loss: 0.21272842586040497, test loss: 0.3447943925857544\n",
      "train loss: 0.21245087683200836, test loss: 0.3439643979072571\n",
      "train loss: 0.2121746838092804, test loss: 0.34313738346099854\n",
      "train loss: 0.21189987659454346, test loss: 0.3423130512237549\n",
      "train loss: 0.21162642538547516, test loss: 0.34149169921875\n",
      "train loss: 0.2113543003797531, test loss: 0.3406729996204376\n",
      "train loss: 0.21108350157737732, test loss: 0.33985719084739685\n",
      "train loss: 0.21081404387950897, test loss: 0.3390440344810486\n",
      "train loss: 0.2105458825826645, test loss: 0.33823373913764954\n",
      "train loss: 0.21027906239032745, test loss: 0.33742621541023254\n",
      "train loss: 0.2100135087966919, test loss: 0.33662155270576477\n",
      "train loss: 0.209749236702919, test loss: 0.3358195722103119\n",
      "train loss: 0.20948627591133118, test loss: 0.33502012491226196\n",
      "train loss: 0.20922456681728363, test loss: 0.33422359824180603\n",
      "train loss: 0.20896413922309875, test loss: 0.33342960476875305\n",
      "train loss: 0.20870493352413177, test loss: 0.332638680934906\n",
      "train loss: 0.20844699442386627, test loss: 0.3318500518798828\n",
      "train loss: 0.20819027721881866, test loss: 0.3310641944408417\n",
      "train loss: 0.20793481171131134, test loss: 0.330281138420105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.20768055319786072, test loss: 0.32950064539909363\n",
      "train loss: 0.2074274718761444, test loss: 0.3287227749824524\n",
      "train loss: 0.207175612449646, test loss: 0.3279477059841156\n",
      "train loss: 0.2069249451160431, test loss: 0.327175110578537\n",
      "train loss: 0.2066754698753357, test loss: 0.3264053463935852\n",
      "train loss: 0.2064271718263626, test loss: 0.3256380259990692\n",
      "train loss: 0.20618005096912384, test loss: 0.32487329840660095\n",
      "train loss: 0.2059340924024582, test loss: 0.3241112530231476\n",
      "train loss: 0.20568929612636566, test loss: 0.32335180044174194\n",
      "train loss: 0.20544564723968506, test loss: 0.3225947916507721\n",
      "train loss: 0.20520314574241638, test loss: 0.32184040546417236\n",
      "train loss: 0.20496176183223724, test loss: 0.32108867168426514\n",
      "train loss: 0.20472148060798645, test loss: 0.320339560508728\n",
      "train loss: 0.20448236167430878, test loss: 0.31959283351898193\n",
      "train loss: 0.20424431562423706, test loss: 0.3188488483428955\n",
      "train loss: 0.20400740206241608, test loss: 0.31810706853866577\n",
      "train loss: 0.20377156138420105, test loss: 0.3173680901527405\n",
      "train loss: 0.20353680849075317, test loss: 0.316631555557251\n",
      "train loss: 0.20330315828323364, test loss: 0.31589755415916443\n",
      "train loss: 0.2030705362558365, test loss: 0.3151659369468689\n",
      "train loss: 0.20283901691436768, test loss: 0.3144368529319763\n",
      "train loss: 0.20260855555534363, test loss: 0.31371021270751953\n",
      "train loss: 0.20237916707992554, test loss: 0.3129861056804657\n",
      "train loss: 0.20215080678462982, test loss: 0.31226447224617004\n",
      "train loss: 0.20192348957061768, test loss: 0.311545193195343\n",
      "train loss: 0.20169717073440552, test loss: 0.31082841753959656\n",
      "train loss: 0.20147192478179932, test loss: 0.31011414527893066\n",
      "train loss: 0.2012476772069931, test loss: 0.3094022572040558\n",
      "train loss: 0.20102445781230927, test loss: 0.3086927831172943\n",
      "train loss: 0.2008022516965866, test loss: 0.30798566341400146\n",
      "train loss: 0.20058104395866394, test loss: 0.30728113651275635\n",
      "train loss: 0.20036081969738007, test loss: 0.30657893419265747\n",
      "train loss: 0.20014160871505737, test loss: 0.30587905645370483\n",
      "train loss: 0.1999233365058899, test loss: 0.3051815330982208\n",
      "train loss: 0.1997060924768448, test loss: 0.304486483335495\n",
      "train loss: 0.1994897872209549, test loss: 0.3037938177585602\n",
      "train loss: 0.1992744356393814, test loss: 0.3031034767627716\n",
      "train loss: 0.1990600824356079, test loss: 0.3024155795574188\n",
      "train loss: 0.19884666800498962, test loss: 0.3017300069332123\n",
      "train loss: 0.19863419234752655, test loss: 0.3010467290878296\n",
      "train loss: 0.19842267036437988, test loss: 0.30036574602127075\n",
      "train loss: 0.19821207225322723, test loss: 0.29968711733818054\n",
      "train loss: 0.1980024129152298, test loss: 0.29901087284088135\n",
      "train loss: 0.19779367744922638, test loss: 0.29833683371543884\n",
      "train loss: 0.1975858509540558, test loss: 0.2976652979850769\n",
      "train loss: 0.1973789483308792, test loss: 0.2969958484172821\n",
      "train loss: 0.19717295467853546, test loss: 0.2963288128376007\n",
      "train loss: 0.19696786999702454, test loss: 0.2956640124320984\n",
      "train loss: 0.19676367938518524, test loss: 0.2950015962123871\n",
      "train loss: 0.1965603530406952, test loss: 0.2943413555622101\n",
      "train loss: 0.19635795056819916, test loss: 0.29368332028388977\n",
      "train loss: 0.19615641236305237, test loss: 0.2930276095867157\n",
      "train loss: 0.19595575332641602, test loss: 0.2923743724822998\n",
      "train loss: 0.1957559436559677, test loss: 0.2917231023311615\n",
      "train loss: 0.19555704295635223, test loss: 0.29107415676116943\n",
      "train loss: 0.1953589916229248, test loss: 0.29042747616767883\n",
      "train loss: 0.19516178965568542, test loss: 0.289782851934433\n",
      "train loss: 0.1949654370546341, test loss: 0.28914061188697815\n",
      "train loss: 0.1947699338197708, test loss: 0.2885006070137024\n",
      "train loss: 0.1945752650499344, test loss: 0.28786271810531616\n",
      "train loss: 0.19438143074512482, test loss: 0.2872270941734314\n",
      "train loss: 0.1941884309053421, test loss: 0.28659358620643616\n",
      "train loss: 0.19399628043174744, test loss: 0.28596237301826477\n",
      "train loss: 0.19380493462085724, test loss: 0.2853332757949829\n",
      "train loss: 0.1936144083738327, test loss: 0.2847062945365906\n",
      "train loss: 0.19342468678951263, test loss: 0.2840817868709564\n",
      "train loss: 0.19323578476905823, test loss: 0.2834591269493103\n",
      "train loss: 0.1930476874113083, test loss: 0.2828387916088104\n",
      "train loss: 0.19286035001277924, test loss: 0.2822205722332001\n",
      "train loss: 0.19267386198043823, test loss: 0.2816043496131897\n",
      "train loss: 0.1924881488084793, test loss: 0.2809903919696808\n",
      "train loss: 0.19230322539806366, test loss: 0.2803786098957062\n",
      "train loss: 0.1921190768480301, test loss: 0.2797689735889435\n",
      "train loss: 0.1919357031583786, test loss: 0.279161274433136\n",
      "train loss: 0.191753089427948, test loss: 0.27855589985847473\n",
      "train loss: 0.19157126545906067, test loss: 0.2779524326324463\n",
      "train loss: 0.19139020144939423, test loss: 0.2773513197898865\n",
      "train loss: 0.19120986759662628, test loss: 0.2767520844936371\n",
      "train loss: 0.1910303384065628, test loss: 0.27615514397621155\n",
      "train loss: 0.19085152447223663, test loss: 0.2755601108074188\n",
      "train loss: 0.19067347049713135, test loss: 0.27496737241744995\n",
      "train loss: 0.19049617648124695, test loss: 0.27437660098075867\n",
      "train loss: 0.19031959772109985, test loss: 0.27378785610198975\n",
      "train loss: 0.19014376401901245, test loss: 0.2732011377811432\n",
      "train loss: 0.18996867537498474, test loss: 0.27261659502983093\n",
      "train loss: 0.18979433178901672, test loss: 0.27203404903411865\n",
      "train loss: 0.18962067365646362, test loss: 0.2714535892009735\n",
      "train loss: 0.18944774568080902, test loss: 0.27087515592575073\n",
      "train loss: 0.18927551805973053, test loss: 0.2702988088130951\n",
      "train loss: 0.18910402059555054, test loss: 0.26972439885139465\n",
      "train loss: 0.18893325328826904, test loss: 0.26915210485458374\n",
      "train loss: 0.18876315653324127, test loss: 0.2685818672180176\n",
      "train loss: 0.1885937750339508, test loss: 0.26801353693008423\n",
      "train loss: 0.18842509388923645, test loss: 0.26744726300239563\n",
      "train loss: 0.188257098197937, test loss: 0.2668830156326294\n",
      "train loss: 0.18808980286121368, test loss: 0.26632076501846313\n",
      "train loss: 0.18792317807674408, test loss: 0.26576048135757446\n",
      "train loss: 0.18775725364685059, test loss: 0.26520228385925293\n",
      "train loss: 0.18759199976921082, test loss: 0.2646459639072418\n",
      "train loss: 0.18742744624614716, test loss: 0.2640916407108307\n",
      "train loss: 0.18726353347301483, test loss: 0.26353931427001953\n",
      "train loss: 0.18710027635097504, test loss: 0.2629889249801636\n",
      "train loss: 0.18693771958351135, test loss: 0.26244059205055237\n",
      "train loss: 0.1867758184671402, test loss: 0.261894166469574\n",
      "train loss: 0.18661458790302277, test loss: 0.2613496780395508\n",
      "train loss: 0.18645398318767548, test loss: 0.26080718636512756\n",
      "train loss: 0.1862940639257431, test loss: 0.26026666164398193\n",
      "train loss: 0.18613477051258087, test loss: 0.25972801446914673\n",
      "train loss: 0.18597611784934998, test loss: 0.2591914236545563\n",
      "train loss: 0.18581810593605042, test loss: 0.25865665078163147\n",
      "train loss: 0.18566074967384338, test loss: 0.25812381505966187\n",
      "train loss: 0.1855040341615677, test loss: 0.2575928568840027\n",
      "train loss: 0.18534795939922333, test loss: 0.25706392526626587\n",
      "train loss: 0.1851925104856491, test loss: 0.2565368115901947\n",
      "train loss: 0.18503765761852264, test loss: 0.2560116648674011\n",
      "train loss: 0.18488344550132751, test loss: 0.25548845529556274\n",
      "train loss: 0.18472984433174133, test loss: 0.25496706366539\n",
      "train loss: 0.1845768839120865, test loss: 0.2544476091861725\n",
      "train loss: 0.1844245195388794, test loss: 0.25393006205558777\n",
      "train loss: 0.18427275121212006, test loss: 0.2534142732620239\n",
      "train loss: 0.18412163853645325, test loss: 0.25290051102638245\n",
      "train loss: 0.1839710921049118, test loss: 0.2523886263370514\n",
      "train loss: 0.1838211715221405, test loss: 0.2518784701824188\n",
      "train loss: 0.18367184698581696, test loss: 0.2513703405857086\n",
      "train loss: 0.18352308869361877, test loss: 0.2508639693260193\n",
      "train loss: 0.18337495625019073, test loss: 0.250359445810318\n",
      "train loss: 0.18322740495204926, test loss: 0.24985681474208832\n",
      "train loss: 0.18308044970035553, test loss: 0.24935607612133026\n",
      "train loss: 0.18293406069278717, test loss: 0.24885721504688263\n",
      "train loss: 0.18278825283050537, test loss: 0.24836000800132751\n",
      "train loss: 0.18264305591583252, test loss: 0.24786485731601715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.18249841034412384, test loss: 0.24737140536308289\n",
      "train loss: 0.18235434591770172, test loss: 0.24687977135181427\n",
      "train loss: 0.18221086263656616, test loss: 0.24639010429382324\n",
      "train loss: 0.18206791579723358, test loss: 0.24590209126472473\n",
      "train loss: 0.18192556500434875, test loss: 0.24541598558425903\n",
      "train loss: 0.1817837804555893, test loss: 0.24493153393268585\n",
      "train loss: 0.1816425323486328, test loss: 0.24444903433322906\n",
      "train loss: 0.1815018653869629, test loss: 0.2439681887626648\n",
      "train loss: 0.18136171996593475, test loss: 0.24348925054073334\n",
      "train loss: 0.18122217059135437, test loss: 0.24301210045814514\n",
      "train loss: 0.18108315765857697, test loss: 0.24253669381141663\n",
      "train loss: 0.18094466626644135, test loss: 0.24206317961215973\n",
      "train loss: 0.18080675601959229, test loss: 0.24159127473831177\n",
      "train loss: 0.180669367313385, test loss: 0.24112127721309662\n",
      "train loss: 0.1805325299501419, test loss: 0.24065302312374115\n",
      "train loss: 0.1803962141275406, test loss: 0.2401864379644394\n",
      "train loss: 0.18026044964790344, test loss: 0.2397216260433197\n",
      "train loss: 0.18012520670890808, test loss: 0.23925866186618805\n",
      "train loss: 0.1799904853105545, test loss: 0.23879744112491608\n",
      "train loss: 0.1798563152551651, test loss: 0.23833784461021423\n",
      "train loss: 0.17972266674041748, test loss: 0.23788012564182281\n",
      "train loss: 0.17958952486515045, test loss: 0.23742417991161346\n",
      "train loss: 0.1794569045305252, test loss: 0.23696985840797424\n",
      "train loss: 0.17932480573654175, test loss: 0.2365172654390335\n",
      "train loss: 0.17919322848320007, test loss: 0.23606646060943604\n",
      "train loss: 0.179062157869339, test loss: 0.23561733961105347\n",
      "train loss: 0.17893162369728088, test loss: 0.23516999185085297\n",
      "train loss: 0.17880158126354218, test loss: 0.23472420871257782\n",
      "train loss: 0.17867204546928406, test loss: 0.2342802733182907\n",
      "train loss: 0.17854300141334534, test loss: 0.2338380366563797\n",
      "train loss: 0.1784144639968872, test loss: 0.2333974391222\n",
      "train loss: 0.17828643321990967, test loss: 0.23295864462852478\n",
      "train loss: 0.17815889418125153, test loss: 0.2325213998556137\n",
      "train loss: 0.17803187668323517, test loss: 0.2320859283208847\n",
      "train loss: 0.17790532112121582, test loss: 0.2316521555185318\n",
      "train loss: 0.17777925729751587, test loss: 0.23122000694274902\n",
      "train loss: 0.1776537150144577, test loss: 0.23078954219818115\n",
      "train loss: 0.17752860486507416, test loss: 0.23036077618598938\n",
      "train loss: 0.1774040311574936, test loss: 0.22993367910385132\n",
      "train loss: 0.17727991938591003, test loss: 0.22950828075408936\n",
      "train loss: 0.17715628445148468, test loss: 0.22908444702625275\n",
      "train loss: 0.17703312635421753, test loss: 0.228662371635437\n",
      "train loss: 0.1769104301929474, test loss: 0.22824186086654663\n",
      "train loss: 0.17678824067115784, test loss: 0.22782307863235474\n",
      "train loss: 0.1766665130853653, test loss: 0.22740590572357178\n",
      "train loss: 0.17654526233673096, test loss: 0.22699034214019775\n",
      "train loss: 0.17642445862293243, test loss: 0.22657643258571625\n",
      "train loss: 0.1763041466474533, test loss: 0.22616416215896606\n",
      "train loss: 0.1761842668056488, test loss: 0.2257535755634308\n",
      "train loss: 0.1760648488998413, test loss: 0.22534450888633728\n",
      "train loss: 0.17594590783119202, test loss: 0.22493726015090942\n",
      "train loss: 0.17582745850086212, test loss: 0.2245313823223114\n",
      "train loss: 0.17570939660072327, test loss: 0.2241271734237671\n",
      "train loss: 0.175591841340065, test loss: 0.22372472286224365\n",
      "train loss: 0.17547473311424255, test loss: 0.2233237326145172\n",
      "train loss: 0.17535804212093353, test loss: 0.22292447090148926\n",
      "train loss: 0.17524182796478271, test loss: 0.22252672910690308\n",
      "train loss: 0.1751260608434677, test loss: 0.22213056683540344\n",
      "train loss: 0.17501071095466614, test loss: 0.22173604369163513\n",
      "train loss: 0.17489582300186157, test loss: 0.2213430106639862\n",
      "train loss: 0.17478136718273163, test loss: 0.220951646566391\n",
      "train loss: 0.1746673583984375, test loss: 0.22056183218955994\n",
      "train loss: 0.174553781747818, test loss: 0.220173642039299\n",
      "train loss: 0.1744406670331955, test loss: 0.21978706121444702\n",
      "train loss: 0.17432795464992523, test loss: 0.21940189599990845\n",
      "train loss: 0.1742156744003296, test loss: 0.2190183848142624\n",
      "train loss: 0.17410381138324738, test loss: 0.2186364233493805\n",
      "train loss: 0.17399241030216217, test loss: 0.21825608611106873\n",
      "train loss: 0.1738814264535904, test loss: 0.21787716448307037\n",
      "train loss: 0.17377085983753204, test loss: 0.21749991178512573\n",
      "train loss: 0.17366071045398712, test loss: 0.2171241194009781\n",
      "train loss: 0.17355097830295563, test loss: 0.216749906539917\n",
      "train loss: 0.17344167828559875, test loss: 0.21637719869613647\n",
      "train loss: 0.1733328104019165, test loss: 0.21600601077079773\n",
      "train loss: 0.1732243299484253, test loss: 0.2156364470720291\n",
      "train loss: 0.1731162667274475, test loss: 0.2152683585882187\n",
      "train loss: 0.17300862073898315, test loss: 0.21490177512168884\n",
      "train loss: 0.17290137708187103, test loss: 0.21453674137592316\n",
      "train loss: 0.17279455065727234, test loss: 0.21417321264743805\n",
      "train loss: 0.17268815636634827, test loss: 0.21381115913391113\n",
      "train loss: 0.17258211970329285, test loss: 0.21345070004463196\n",
      "train loss: 0.17247651517391205, test loss: 0.21309171617031097\n",
      "train loss: 0.17237131297588348, test loss: 0.2127341479063034\n",
      "train loss: 0.17226649820804596, test loss: 0.21237818896770477\n",
      "train loss: 0.17216210067272186, test loss: 0.21202367544174194\n",
      "train loss: 0.1720580905675888, test loss: 0.2116706669330597\n",
      "train loss: 0.17195449769496918, test loss: 0.21131908893585205\n",
      "train loss: 0.1718512773513794, test loss: 0.210969015955925\n",
      "train loss: 0.17174842953681946, test loss: 0.2106204330921173\n",
      "train loss: 0.17164599895477295, test loss: 0.2102733701467514\n",
      "train loss: 0.17154397070407867, test loss: 0.2099277824163437\n",
      "train loss: 0.17144231498241425, test loss: 0.20958364009857178\n",
      "train loss: 0.17134103178977966, test loss: 0.20924091339111328\n",
      "train loss: 0.17124013602733612, test loss: 0.20889967679977417\n",
      "train loss: 0.17113962769508362, test loss: 0.20855990052223206\n",
      "train loss: 0.17103952169418335, test loss: 0.20822161436080933\n",
      "train loss: 0.17093975841999054, test loss: 0.20788468420505524\n",
      "train loss: 0.17084041237831116, test loss: 0.20754924416542053\n",
      "train loss: 0.17074142396450043, test loss: 0.20721521973609924\n",
      "train loss: 0.17064280807971954, test loss: 0.20688268542289734\n",
      "train loss: 0.1705445647239685, test loss: 0.20655161142349243\n",
      "train loss: 0.1704467087984085, test loss: 0.20622190833091736\n",
      "train loss: 0.17034921050071716, test loss: 0.20589374005794525\n",
      "train loss: 0.17025206983089447, test loss: 0.20556682348251343\n",
      "train loss: 0.17015530169010162, test loss: 0.2052413821220398\n",
      "train loss: 0.170058935880661, test loss: 0.20491750538349152\n",
      "train loss: 0.16996289789676666, test loss: 0.20459485054016113\n",
      "train loss: 0.16986723244190216, test loss: 0.2042737603187561\n",
      "train loss: 0.16977190971374512, test loss: 0.20395396649837494\n",
      "train loss: 0.16967695951461792, test loss: 0.20363560318946838\n",
      "train loss: 0.16958241164684296, test loss: 0.20331868529319763\n",
      "train loss: 0.16948817670345306, test loss: 0.20300309360027313\n",
      "train loss: 0.16939429938793182, test loss: 0.20268899202346802\n",
      "train loss: 0.16930077970027924, test loss: 0.202376127243042\n",
      "train loss: 0.1692076176404953, test loss: 0.20206472277641296\n",
      "train loss: 0.16911481320858002, test loss: 0.20175474882125854\n",
      "train loss: 0.1690223664045334, test loss: 0.20144613087177277\n",
      "train loss: 0.16893024742603302, test loss: 0.2011389136314392\n",
      "train loss: 0.1688385009765625, test loss: 0.2008330523967743\n",
      "train loss: 0.16874706745147705, test loss: 0.20052848756313324\n",
      "train loss: 0.16865602135658264, test loss: 0.2002253234386444\n",
      "train loss: 0.1685652732849121, test loss: 0.199923574924469\n",
      "train loss: 0.16847489774227142, test loss: 0.19962316751480103\n",
      "train loss: 0.1683848351240158, test loss: 0.1993240863084793\n",
      "train loss: 0.16829514503479004, test loss: 0.19902637600898743\n",
      "train loss: 0.16820576786994934, test loss: 0.19873003661632538\n",
      "train loss: 0.1681167483329773, test loss: 0.198434978723526\n",
      "train loss: 0.16802802681922913, test loss: 0.19814127683639526\n",
      "train loss: 0.1679396778345108, test loss: 0.19784891605377197\n",
      "train loss: 0.16785165667533875, test loss: 0.1975579410791397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.16776396334171295, test loss: 0.19726823270320892\n",
      "train loss: 0.16767659783363342, test loss: 0.19697992503643036\n",
      "train loss: 0.16758956015110016, test loss: 0.19669285416603088\n",
      "train loss: 0.16750288009643555, test loss: 0.19640712440013885\n",
      "train loss: 0.167416512966156, test loss: 0.19612273573875427\n",
      "train loss: 0.16733042895793915, test loss: 0.19583962857723236\n",
      "train loss: 0.16724471747875214, test loss: 0.1955578774213791\n",
      "train loss: 0.1671593189239502, test loss: 0.19527745246887207\n",
      "train loss: 0.16707424819469452, test loss: 0.19499823451042175\n",
      "train loss: 0.1669894903898239, test loss: 0.19472041726112366\n",
      "train loss: 0.16690503060817719, test loss: 0.19444383680820465\n",
      "train loss: 0.16682091355323792, test loss: 0.19416852295398712\n",
      "train loss: 0.16673710942268372, test loss: 0.19389458000659943\n",
      "train loss: 0.1666536033153534, test loss: 0.19362187385559082\n",
      "train loss: 0.16657042503356934, test loss: 0.19335044920444489\n",
      "train loss: 0.16648755967617035, test loss: 0.19308030605316162\n",
      "train loss: 0.16640502214431763, test loss: 0.19281141459941864\n",
      "train loss: 0.1663227677345276, test loss: 0.1925438791513443\n",
      "train loss: 0.1662408411502838, test loss: 0.19227753579616547\n",
      "train loss: 0.1661592274904251, test loss: 0.19201257824897766\n",
      "train loss: 0.16607791185379028, test loss: 0.1917487531900406\n",
      "train loss: 0.16599687933921814, test loss: 0.19148625433444977\n",
      "train loss: 0.16591621935367584, test loss: 0.19122494757175446\n",
      "train loss: 0.16583579778671265, test loss: 0.19096492230892181\n",
      "train loss: 0.16575570404529572, test loss: 0.19070616364479065\n",
      "train loss: 0.16567589342594147, test loss: 0.19044864177703857\n",
      "train loss: 0.16559641063213348, test loss: 0.19019237160682678\n",
      "train loss: 0.165517196059227, test loss: 0.1899372637271881\n",
      "train loss: 0.16543832421302795, test loss: 0.18968351185321808\n",
      "train loss: 0.1653597056865692, test loss: 0.18943092226982117\n",
      "train loss: 0.16528140008449554, test loss: 0.18917958438396454\n",
      "train loss: 0.16520339250564575, test loss: 0.1889294683933258\n",
      "train loss: 0.16512568295001984, test loss: 0.18868055939674377\n",
      "train loss: 0.1650482416152954, test loss: 0.18843290209770203\n",
      "train loss: 0.16497109830379486, test loss: 0.18818646669387817\n",
      "train loss: 0.1648942530155182, test loss: 0.18794117867946625\n",
      "train loss: 0.1648177057504654, test loss: 0.18769709765911102\n",
      "train loss: 0.16474144160747528, test loss: 0.18745426833629608\n",
      "train loss: 0.16466546058654785, test loss: 0.18721264600753784\n",
      "train loss: 0.1645897626876831, test loss: 0.1869722157716751\n",
      "train loss: 0.16451436281204224, test loss: 0.18673297762870789\n",
      "train loss: 0.16443923115730286, test loss: 0.18649494647979736\n",
      "train loss: 0.16436438262462616, test loss: 0.18625812232494354\n",
      "train loss: 0.16428981721401215, test loss: 0.18602238595485687\n",
      "train loss: 0.164215549826622, test loss: 0.18578794598579407\n",
      "train loss: 0.16414153575897217, test loss: 0.185554638504982\n",
      "train loss: 0.164067804813385, test loss: 0.18532249331474304\n",
      "train loss: 0.16399437189102173, test loss: 0.18509157001972198\n",
      "train loss: 0.16392120718955994, test loss: 0.18486179411411285\n",
      "train loss: 0.16384829580783844, test loss: 0.18463316559791565\n",
      "train loss: 0.16377566754817963, test loss: 0.18440569937229156\n",
      "train loss: 0.1637033224105835, test loss: 0.1841793954372406\n",
      "train loss: 0.16363123059272766, test loss: 0.18395423889160156\n",
      "train loss: 0.1635594218969345, test loss: 0.18373025953769684\n",
      "train loss: 0.16348789632320404, test loss: 0.18350739777088165\n",
      "train loss: 0.16341662406921387, test loss: 0.18328571319580078\n",
      "train loss: 0.16334563493728638, test loss: 0.18306517601013184\n",
      "train loss: 0.16327491402626038, test loss: 0.1828458160161972\n",
      "train loss: 0.16320443153381348, test loss: 0.18262752890586853\n",
      "train loss: 0.16313423216342926, test loss: 0.18241041898727417\n",
      "train loss: 0.16306430101394653, test loss: 0.18219441175460815\n",
      "train loss: 0.1629946231842041, test loss: 0.18197952210903168\n",
      "train loss: 0.16292518377304077, test loss: 0.18176576495170593\n",
      "train loss: 0.16285602748394012, test loss: 0.18155311048030853\n",
      "train loss: 0.16278713941574097, test loss: 0.18134163320064545\n",
      "train loss: 0.1627185195684433, test loss: 0.18113122880458832\n",
      "train loss: 0.16265013813972473, test loss: 0.18092197179794312\n",
      "train loss: 0.16258202493190765, test loss: 0.18071380257606506\n",
      "train loss: 0.16251416504383087, test loss: 0.18050672113895416\n",
      "train loss: 0.1624465435743332, test loss: 0.1803007572889328\n",
      "train loss: 0.1623792052268982, test loss: 0.1800958514213562\n",
      "train loss: 0.1623120903968811, test loss: 0.17989207804203033\n",
      "train loss: 0.1622452437877655, test loss: 0.17968939244747162\n",
      "train loss: 0.162178635597229, test loss: 0.17948779463768005\n",
      "train loss: 0.1621123105287552, test loss: 0.17928731441497803\n",
      "train loss: 0.1620461791753769, test loss: 0.17908789217472076\n",
      "train loss: 0.16198034584522247, test loss: 0.17888951301574707\n",
      "train loss: 0.16191473603248596, test loss: 0.17869225144386292\n",
      "train loss: 0.16184936463832855, test loss: 0.17849604785442352\n",
      "train loss: 0.16178426146507263, test loss: 0.17830094695091248\n",
      "train loss: 0.161719411611557, test loss: 0.1781068593263626\n",
      "train loss: 0.1616547703742981, test loss: 0.1779138594865799\n",
      "train loss: 0.16159041225910187, test loss: 0.17772190272808075\n",
      "train loss: 0.16152626276016235, test loss: 0.17753103375434875\n",
      "train loss: 0.16146236658096313, test loss: 0.17734120786190033\n",
      "train loss: 0.16139870882034302, test loss: 0.17715243995189667\n",
      "train loss: 0.161335289478302, test loss: 0.17696471512317657\n",
      "train loss: 0.1612721085548401, test loss: 0.17677798867225647\n",
      "train loss: 0.16120915114879608, test loss: 0.1765923798084259\n",
      "train loss: 0.16114644706249237, test loss: 0.17640776932239532\n",
      "train loss: 0.16108399629592896, test loss: 0.1762242317199707\n",
      "train loss: 0.16102173924446106, test loss: 0.1760416477918625\n",
      "train loss: 0.16095973551273346, test loss: 0.17586015164852142\n",
      "train loss: 0.16089798510074615, test loss: 0.17567968368530273\n",
      "train loss: 0.16083642840385437, test loss: 0.17550021409988403\n",
      "train loss: 0.16077513992786407, test loss: 0.1753217875957489\n",
      "train loss: 0.1607140451669693, test loss: 0.17514434456825256\n",
      "train loss: 0.16065320372581482, test loss: 0.1749679297208786\n",
      "train loss: 0.16059257090091705, test loss: 0.17479251325130463\n",
      "train loss: 0.1605321764945984, test loss: 0.17461813986301422\n",
      "train loss: 0.16047202050685883, test loss: 0.17444473505020142\n",
      "train loss: 0.16041207313537598, test loss: 0.174272358417511\n",
      "train loss: 0.16035236418247223, test loss: 0.17410093545913696\n",
      "train loss: 0.16029289364814758, test loss: 0.17393054068088531\n",
      "train loss: 0.16023361682891846, test loss: 0.17376118898391724\n",
      "train loss: 0.16017456352710724, test loss: 0.17359277606010437\n",
      "train loss: 0.16011574864387512, test loss: 0.1734253317117691\n",
      "train loss: 0.16005714237689972, test loss: 0.17325887084007263\n",
      "train loss: 0.15999877452850342, test loss: 0.17309343814849854\n",
      "train loss: 0.15994061529636383, test loss: 0.17292891442775726\n",
      "train loss: 0.15988269448280334, test loss: 0.17276543378829956\n",
      "train loss: 0.15982495248317719, test loss: 0.17260290682315826\n",
      "train loss: 0.15976743400096893, test loss: 0.172441303730011\n",
      "train loss: 0.1597101390361786, test loss: 0.1722807139158249\n",
      "train loss: 0.15965308248996735, test loss: 0.17212101817131042\n",
      "train loss: 0.15959621965885162, test loss: 0.17196232080459595\n",
      "train loss: 0.15953956544399261, test loss: 0.17180460691452026\n",
      "train loss: 0.1594831347465515, test loss: 0.17164777219295502\n",
      "train loss: 0.15942691266536713, test loss: 0.17149193584918976\n",
      "train loss: 0.15937088429927826, test loss: 0.17133702337741852\n",
      "train loss: 0.1593150943517685, test loss: 0.17118307948112488\n",
      "train loss: 0.15925949811935425, test loss: 0.17103008925914764\n",
      "train loss: 0.15920411050319672, test loss: 0.17087797820568085\n",
      "train loss: 0.1591489315032959, test loss: 0.17072683572769165\n",
      "train loss: 0.1590939611196518, test loss: 0.17057664692401886\n",
      "train loss: 0.1590392142534256, test loss: 0.1704273372888565\n",
      "train loss: 0.15898463129997253, test loss: 0.17027896642684937\n",
      "train loss: 0.15893028676509857, test loss: 0.17013154923915863\n",
      "train loss: 0.15887615084648132, test loss: 0.16998499631881714\n",
      "train loss: 0.1588221937417984, test loss: 0.16983938217163086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.1587684452533722, test loss: 0.169694721698761\n",
      "train loss: 0.1587149053812027, test loss: 0.16955094039440155\n",
      "train loss: 0.15866157412528992, test loss: 0.16940808296203613\n",
      "train loss: 0.15860843658447266, test loss: 0.16926607489585876\n",
      "train loss: 0.15855549275875092, test loss: 0.169125035405159\n",
      "train loss: 0.1585027575492859, test loss: 0.16898487508296967\n",
      "train loss: 0.15845021605491638, test loss: 0.16884556412696838\n",
      "train loss: 0.1583978533744812, test loss: 0.16870716214179993\n",
      "train loss: 0.15834569931030273, test loss: 0.1685696691274643\n",
      "train loss: 0.15829375386238098, test loss: 0.1684330701828003\n",
      "train loss: 0.15824200212955475, test loss: 0.16829733550548553\n",
      "train loss: 0.15819044411182404, test loss: 0.16816245019435883\n",
      "train loss: 0.15813905000686646, test loss: 0.16802851855754852\n",
      "train loss: 0.1580878645181656, test loss: 0.16789540648460388\n",
      "train loss: 0.15803688764572144, test loss: 0.16776320338249207\n",
      "train loss: 0.15798607468605042, test loss: 0.1676318198442459\n",
      "train loss: 0.1579354852437973, test loss: 0.1675013303756714\n",
      "train loss: 0.15788505971431732, test loss: 0.1673716902732849\n",
      "train loss: 0.15783484280109406, test loss: 0.1672428846359253\n",
      "train loss: 0.15778480470180511, test loss: 0.1671149879693985\n",
      "train loss: 0.1577349454164505, test loss: 0.16698788106441498\n",
      "train loss: 0.1576852798461914, test loss: 0.16686168313026428\n",
      "train loss: 0.15763582289218903, test loss: 0.16673631966114044\n",
      "train loss: 0.15758651494979858, test loss: 0.16661177575588226\n",
      "train loss: 0.15753741562366486, test loss: 0.16648811101913452\n",
      "train loss: 0.15748848021030426, test loss: 0.16636528074741364\n",
      "train loss: 0.15743973851203918, test loss: 0.16624324023723602\n",
      "train loss: 0.15739119052886963, test loss: 0.16612207889556885\n",
      "train loss: 0.1573428213596344, test loss: 0.16600172221660614\n",
      "train loss: 0.1572946161031723, test loss: 0.16588224470615387\n",
      "train loss: 0.15724658966064453, test loss: 0.16576354205608368\n",
      "train loss: 0.15719877183437347, test loss: 0.16564565896987915\n",
      "train loss: 0.15715110301971436, test loss: 0.16552861034870148\n",
      "train loss: 0.15710364282131195, test loss: 0.16541236639022827\n",
      "train loss: 0.15705636143684387, test loss: 0.16529692709445953\n",
      "train loss: 0.15700924396514893, test loss: 0.16518235206604004\n",
      "train loss: 0.1569623053073883, test loss: 0.16506850719451904\n",
      "train loss: 0.15691553056240082, test loss: 0.1649555265903473\n",
      "train loss: 0.15686893463134766, test loss: 0.1648433357477188\n",
      "train loss: 0.15682250261306763, test loss: 0.1647319197654724\n",
      "train loss: 0.15677626430988312, test loss: 0.1646212935447693\n",
      "train loss: 0.15673018991947174, test loss: 0.16451147198677063\n",
      "train loss: 0.1566842943429947, test loss: 0.16440245509147644\n",
      "train loss: 0.15663856267929077, test loss: 0.16429422795772552\n",
      "train loss: 0.15659302473068237, test loss: 0.1641867756843567\n",
      "train loss: 0.1565476506948471, test loss: 0.16408009827136993\n",
      "train loss: 0.15650244057178497, test loss: 0.16397424042224884\n",
      "train loss: 0.15645737946033478, test loss: 0.16386911273002625\n",
      "train loss: 0.1564125269651413, test loss: 0.16376477479934692\n",
      "train loss: 0.15636780858039856, test loss: 0.16366124153137207\n",
      "train loss: 0.15632326900959015, test loss: 0.16355840861797333\n",
      "train loss: 0.15627890825271606, test loss: 0.16345636546611786\n",
      "train loss: 0.15623468160629272, test loss: 0.16335506737232208\n",
      "train loss: 0.1561906337738037, test loss: 0.16325455904006958\n",
      "train loss: 0.15614676475524902, test loss: 0.16315481066703796\n",
      "train loss: 0.15610305964946747, test loss: 0.16305580735206604\n",
      "train loss: 0.15605950355529785, test loss: 0.16295751929283142\n",
      "train loss: 0.15601612627506256, test loss: 0.16286005079746246\n",
      "train loss: 0.15597288310527802, test loss: 0.16276326775550842\n",
      "train loss: 0.15592984855175018, test loss: 0.16266724467277527\n",
      "train loss: 0.1558869183063507, test loss: 0.1625719666481018\n",
      "train loss: 0.15584416687488556, test loss: 0.16247740387916565\n",
      "train loss: 0.15580157935619354, test loss: 0.16238361597061157\n",
      "train loss: 0.15575915575027466, test loss: 0.1622905284166336\n",
      "train loss: 0.1557168960571289, test loss: 0.16219818592071533\n",
      "train loss: 0.1556748002767563, test loss: 0.16210654377937317\n",
      "train loss: 0.1556328535079956, test loss: 0.1620156466960907\n",
      "train loss: 0.15559105575084686, test loss: 0.16192546486854553\n",
      "train loss: 0.15554942190647125, test loss: 0.16183601319789886\n",
      "train loss: 0.15550793707370758, test loss: 0.1617472618818283\n",
      "train loss: 0.15546663105487823, test loss: 0.16165922582149506\n",
      "train loss: 0.15542545914649963, test loss: 0.1615719199180603\n",
      "train loss: 0.15538445115089417, test loss: 0.16148529946804047\n",
      "train loss: 0.15534354746341705, test loss: 0.16139934957027435\n",
      "train loss: 0.15530285239219666, test loss: 0.16131415963172913\n",
      "train loss: 0.1552623063325882, test loss: 0.16122962534427643\n",
      "train loss: 0.15522189438343048, test loss: 0.16114585101604462\n",
      "train loss: 0.1551816314458847, test loss: 0.16106271743774414\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "model.teach(train_features, train_prediction, test_features, test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.611597130734467"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_predictions = model.predict_positions(test_pool.features)\n",
    "metric(position_predictions, test_pool.positions, test_pool.targets, test_pool.probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
